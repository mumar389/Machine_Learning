{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayse Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since Iris dataset is continous we need to make discrete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_iris()\n",
    "x=data.data\n",
    "y=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making data discrete\n",
    "def makeLabelled(column):\n",
    "    second_limit = column.mean ( )\n",
    "    first_limit = 0.5 * second_limit\n",
    "    third_limit = 1.5*second_limit\n",
    "    for i in range (0, len(column)):\n",
    "        if (column[i] < first_limit):\n",
    "            column[i] = 0\n",
    "        elif (column[i] < second_limit):\n",
    "            column[i] = 1\n",
    "        elif(column[i] < third_limit):\n",
    "            column[i] =2\n",
    "        else:\n",
    "            column[i] =3\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,x.shape[-1]):\n",
    "    x[:,i]=makeLabelled(x[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_1=np.array([[\"High\",\"High\",\"Mid\",\"High\",\"Low\",\"Low\",\"Low\",\"Mid\",\"Low\",\"Low\"],[\"Job\",\"Bus\",\"Bus\",\"Bus\",\"Bus\",\"Bus\",\"Job\",\"Job\",\"Job\",\"Job\"]])\n",
    "# x=x_1.transpose()\n",
    "# y=np.array([\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"No\",\"No\",\"No\",\"No\",\"No\"])\n",
    "# x_test_1=np.array([[\"High\",\"High\",\"Mid\",\"High\",\"Low\",\"Low\",\"Low\",\"Mid\",\"Low\",\"Low\"],[\"Job\",\"Bus\",\"Bus\",\"Bus\",\"Bus\",\"Bus\",\"Job\",\"Job\",\"Job\",\"Job\"]])\n",
    "# x_test=x_test_1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train,y_train):\n",
    "    result={}\n",
    "    class_values=set(y_train)#getting the unique classes in y i.e ai\n",
    "    #each class will be a key of result dictionary having all the feture numbers\n",
    "    for current_class in class_values:\n",
    "        result[current_class]={}\n",
    "        result[\"total_data\"]=len(y_train)\n",
    "        current_class_crows=(y_train==current_class)\n",
    "        x_train_current=x_train[current_class_crows]\n",
    "        y_train_current=y_train[current_class_crows]\n",
    "        num_features=x_train.shape[1]\n",
    "        result[current_class][\"total_count\"]=len(y_train_current)\n",
    "        #all possible values that feature j can take, will be key and it will store count\n",
    "        for j in range(1,num_features+1):\n",
    "            result[current_class][j]={}\n",
    "            all_possible_values=set(x_train[:,j-1])\n",
    "            for cv in all_possible_values:\n",
    "                result[current_class][j][cv]=(x_train_current[:,j-1]==cv).sum()  #we want to store count\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def probability(result,x,cc):\n",
    "#     num_features=len(result[cc].keys())-1\n",
    "#     output=result[cc]['total_count']/result['total_data']\n",
    "#     for j in range(1,num_features+1):\n",
    "#             xj=x[j-1]\n",
    "#             count_xj_ai=result[cc][j][xj]+1\n",
    "#             count_ai=result[cc][\"total_count\"]+len(result[cc][j].keys())\n",
    "#             prob=count_xj_ai/count_ai\n",
    "#             output=output*prob\n",
    "#     return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting log probabilities\n",
    "def probability(result,x,cc):\n",
    "    num_features=len(result[cc].keys())-1\n",
    "    output=np.log(result[cc]['total_count'])-np.log(result['total_data'])\n",
    "    for j in range(1,num_features+1):\n",
    "            xj=x[j-1]\n",
    "            count_xj_ai=result[cc][j][xj]+1\n",
    "            count_ai=result[cc][\"total_count\"]+len(result[cc][j].keys())\n",
    "            prob=np.log(count_xj_ai)-np.log(count_ai)\n",
    "            output=output+prob\n",
    "    return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(result,x):\n",
    "    classes=result.keys()\n",
    "    best_p=-1000\n",
    "    best_class=-1\n",
    "    for cc in classes:\n",
    "        if(cc=='total_data'):\n",
    "            continue\n",
    "        p_current_class=probability(result,x,cc)\n",
    "        first_run=True\n",
    "        if(first_run or p_current_class>best_p):\n",
    "            best_p=p_current_class\n",
    "            best_class=cc\n",
    "            first_run=False\n",
    "    return best_class        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(result,x_test):\n",
    "    y_pred=[]\n",
    "    for x in x_test:\n",
    "        x_class=predictSinglePoint(result,x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'total_count': 37, 1: {1.0: 37, 2.0: 0}, 2: {1.0: 6, 2.0: 31}, 3: {0.0: 37, 1.0: 0, 2.0: 0, 3.0: 0}, 4: {0.0: 36, 1.0: 1, 2.0: 0, 3.0: 0}}, 'total_data': 112, 1: {'total_count': 34, 1: {1.0: 19, 2.0: 15}, 2: {1.0: 28, 2.0: 6}, 3: {0.0: 0, 1.0: 6, 2.0: 28, 3.0: 0}, 4: {0.0: 0, 1.0: 8, 2.0: 25, 3.0: 1}}, 2: {'total_count': 41, 1: {1.0: 4, 2.0: 37}, 2: {1.0: 26, 2.0: 15}, 3: {0.0: 0, 1.0: 0, 2.0: 24, 3.0: 17}, 4: {0.0: 0, 1.0: 0, 2.0: 4, 3.0: 37}}}\n"
     ]
    }
   ],
   "source": [
    "res=fit(x_train,y_train)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=predict(res,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.24      1.00      0.38         9\n",
      "\n",
      "    accuracy                           0.24        38\n",
      "   macro avg       0.08      0.33      0.13        38\n",
      "weighted avg       0.06      0.24      0.09        38\n",
      "\n",
      "[[ 0  0 13]\n",
      " [ 0  0 16]\n",
      " [ 0  0  9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(y_test,Y_pred))\n",
    "print(confusion_matrix(y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
